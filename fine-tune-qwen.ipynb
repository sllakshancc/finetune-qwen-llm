{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83_3GPGhxY57"
   },
   "source": [
    "## 1. Data Generation & Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfraKSxByBL9"
   },
   "source": [
    "### 1.1. Data Loading and Preprocessing\n",
    "\n",
    "make sure to upload md and pdf files into `/data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FjXHjHKyDfQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def load_and_preprocess_data(data_dir):\n",
    "    \"\"\"Loads and preprocesses Markdown and PDF files.\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if filename.endswith(\".md\"):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                text = re.sub(r\"^[#\\s]+\", \"\", text, flags=re.MULTILINE)\n",
    "                documents.append({\"filename\": filename, \"text\": text})\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            try:\n",
    "                with open(filepath, \"rb\") as f:\n",
    "                    pdf_reader = PdfReader(f)\n",
    "                    text = \"\"\n",
    "                    for page in pdf_reader.pages:\n",
    "                        text += page.extract_text()\n",
    "                    # cleaning\n",
    "                    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "                    text = re.sub(r\" +\", \" \", text)\n",
    "                    documents.append({\"filename\": filename, \"text\": text})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading PDF {filename}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "data_dir = \"data\"\n",
    "documents = load_and_preprocess_data(data_dir)\n",
    "# print(documents[0]['text'][:500]) # check a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqxY2vVV0lk3"
   },
   "source": [
    "### 1.2. Synthetic Data Generation (using Qwen2.5-3B-Instruct itself, initially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jh0ELrAO0pQ0"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "def generate_qa_pairs(documents, model_name=\"Qwen/Qwen2.5-3B-Instruct\", num_questions_per_doc=5):\n",
    "    \"\"\"Generates QA pairs using the base Qwen model.\"\"\"\n",
    "\n",
    "    generator = pipeline('text-generation', model=model_name, device=0)  # Use GPU if available\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    qa_pairs = []\n",
    "    for doc in documents:\n",
    "\n",
    "        text_chunks = [doc['text'][i:i+4096] for i in range(0, len(doc['text']), 4096)]\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            prompt = f\"\"\"\n",
    "            Context:\n",
    "            {chunk}\n",
    "\n",
    "            Based on the above context, generate {num_questions_per_doc} question and answer pairs.\n",
    "            Format them strictly as follows:\n",
    "\n",
    "            Q: [Question 1]\n",
    "            A: [Answer 1]\n",
    "\n",
    "            ...\n",
    "            \"\"\"\n",
    "\n",
    "            # Generate text using model\n",
    "            generated_text = generator(\n",
    "                prompt,\n",
    "                max_length=1024,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7\n",
    "            )[0]['generated_text']\n",
    "\n",
    "\n",
    "            matches = re.findall(r\"Q: (.*?)\\nA: (.*?)(?=\\nQ:|\\Z)\", generated_text, re.DOTALL)\n",
    "            for question, answer in matches:\n",
    "                qa_pairs.append({\"question\": question.strip(), \"answer\": answer.strip(), \"source\": doc['filename']})\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "initial_qa_pairs = generate_qa_pairs(documents, num_questions_per_doc=3)\n",
    "# print(initial_qa_pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBpkmB1J1ITt"
   },
   "source": [
    "### 1.3. Data Augmentation and Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8AhWD-k1VL1"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def paraphrase_qa(qa_pairs, model_name=\"google/flan-t5-base\"):\n",
    "  \"\"\"Paraphrases questions and answers for data augmentation.\"\"\"\n",
    "  paraphraser = pipeline(\"text2text-generation\", model=model_name, device=0) # Use GPU\n",
    "  augmented_qa_pairs = []\n",
    "\n",
    "  for pair in qa_pairs:\n",
    "\n",
    "    prompt_q = f\"paraphrase: {pair['question']}\"\n",
    "    paraphrased_question = paraphraser(prompt_q, max_length=128, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)[0]['generated_text']\n",
    "\n",
    "\n",
    "    prompt_a = f\"paraphrase: {pair['answer']}\"\n",
    "    paraphrased_answer = paraphraser(prompt_a, max_length=256, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)[0]['generated_text']\n",
    "\n",
    "    augmented_qa_pairs.append({\n",
    "      \"question\": paraphrased_question.strip(),\n",
    "      \"answer\": paraphrased_answer.strip(),\n",
    "      \"original_question\": pair['question'],\n",
    "      \"original_answer\": pair['answer'],\n",
    "      \"source\": pair['source']\n",
    "    })\n",
    "  return augmented_qa_pairs\n",
    "\n",
    "augmented_pairs = paraphrase_qa(initial_qa_pairs)\n",
    "all_qa_pairs = initial_qa_pairs + augmented_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il4khaCg1jTP"
   },
   "source": [
    "### 1.4. Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHX6VMBI1mCE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(qa_pairs, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"Splits the dataset into training, validation, and test sets.\"\"\"\n",
    "    random.shuffle(qa_pairs)\n",
    "    train_size = int(len(qa_pairs) * train_ratio)\n",
    "    val_size = int(len(qa_pairs) * val_ratio)\n",
    "    train_data = qa_pairs[:train_size]\n",
    "    val_data = qa_pairs[train_size:train_size + val_size]\n",
    "    test_data = qa_pairs[train_size + val_size:]\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data, val_data, test_data = split_dataset(all_qa_pairs)\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB7ZnTiy2tze"
   },
   "source": [
    "### 1.5 Dataset Formatting (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKHz3ePj2yK9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_jsonl_dataset(qa_pairs, filename):\n",
    "    \"\"\"Creates a JSONL dataset file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for pair in qa_pairs:\n",
    "           prompt = f\"Question: {pair['question']}\\n\"\n",
    "           if 'context' in pair:\n",
    "             prompt += f\"Context: {pair['context']}\\n\"\n",
    "\n",
    "           # Create the JSON object\n",
    "           data_point = {\n",
    "               \"prompt\": prompt,\n",
    "               \"response\": pair['answer']\n",
    "           }\n",
    "           f.write(json.dumps(data_point) + \"\\n\")\n",
    "\n",
    "create_jsonl_dataset(train_data, \"train.jsonl\")\n",
    "create_jsonl_dataset(val_data, \"val.jsonl\")\n",
    "create_jsonl_dataset(test_data, \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNxrpTNK3MWY"
   },
   "source": [
    "## 2. Model Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeFOthn53QWH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Quantization Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Use \"cuda\" if you have a GPU, \"cpu\" otherwise\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhvWmsQE3iEn"
   },
   "source": [
    "## 3. Efficient Fine-tuning (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_h6G-EU3kZu"
   },
   "outputs": [],
   "source": [
    "pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDnwmbOK3nJs"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    ")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=\"val.jsonl\", split=\"train\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",  # Output directory\n",
    "    per_device_train_batch_size=4,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=4,   # Accumulate gradients over several steps\n",
    "    learning_rate=2e-4,             # Learning rate\n",
    "    fp16=False,                     # Use bfloat16 (more stable, better for Qwen2)\n",
    "    bf16=True,\n",
    "    logging_steps=10,             # Log training information\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",      # Evaluate during training\n",
    "    eval_steps=100,                # Evaluation interval\n",
    "    num_train_epochs=3,          # Number of training epochs (adjust as needed)\n",
    "    warmup_steps=100,              # Warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type=\"cosine\",      # Learning rate scheduler\n",
    "    remove_unused_columns=False,    # Important!\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    packing=False,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapters\n",
    "trainer.save_model(\"./qwen-finetuned-adapters\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5JLxfDcMw9zImAofRWJLB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
